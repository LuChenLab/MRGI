{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "from onto_vae.ontobj import *\n",
    "from onto_vae.vae_model import *\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from onto_vae.modules import Encoder, Decoder, OntoEncoder, OntoDecoder\n",
    "from onto_vae.fast_data_loader import FastTensorDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Ontology object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"..data/GO_symbol_ontobj.pickle\",\"rb+\")\n",
    "onto_obj=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_obj.match_dataset(expr_data ='../data/expr_vst_combat.csv',\n",
    "                  name='immune_cell',\n",
    "                  top_thresh=1000, \n",
    "                   bottom_thresh=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OntoVAE model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "###-------------------------------------------------------------###\n",
    "##                  VAE WITH ONTOLOGY IN DECODER                 ##\n",
    "###-------------------------------------------------------------###\n",
    "\n",
    "class OntoVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    This class combines a normal encoder with an ontology structured decoder\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    ontobj: instance of the class Ontobj(), containing a preprocessed ontology and the training data\n",
    "    dataset: which dataset to use for training\n",
    "    top_thresh: top threshold to tell which trimmed ontology to use\n",
    "    bottom_thresh: bottom_threshold to tell which trimmed ontology to use\n",
    "    neuronnum: number of neurons per term\n",
    "    drop: dropout rate, default is 0.2\n",
    "    z_drop: dropout rate for latent space, default is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ontobj, dataset, top_thresh=1000, bottom_thresh=30, neuronnum=3, drop=0.2, z_drop=0.5):\n",
    "        super(OntoVAE, self).__init__()\n",
    "\n",
    "        if not str(top_thresh) + '_' + str(bottom_thresh) in ontobj.genes.keys():\n",
    "            sys.exit('Available trimming thresholds are: ' + ', '.join(list(ontobj.genes.keys())))\n",
    "\n",
    "        self.ontology = ontobj.description\n",
    "        self.top = top_thresh\n",
    "        self.bottom = bottom_thresh\n",
    "        self.genes = ontobj.genes[str(top_thresh) + '_' + str(bottom_thresh)]\n",
    "        self.in_features = len(self.genes)\n",
    "        self.mask_list = ontobj.masks[str(top_thresh) + '_' + str(bottom_thresh)]\n",
    "        self.mask_list = [torch.tensor(m, dtype=torch.float32) for m in self.mask_list]\n",
    "        self.layer_dims_dec =  np.array([self.mask_list[0].shape[1]] + [m.shape[0] for m in self.mask_list])\n",
    "        self.latent_dim = self.layer_dims_dec[0] * neuronnum\n",
    "        self.layer_dims_enc = [self.latent_dim]\n",
    "        self.neuronnum = neuronnum\n",
    "        self.drop = drop\n",
    "        self.z_drop = z_drop\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(self.in_features,\n",
    "                                self.latent_dim,\n",
    "                                self.layer_dims_enc,\n",
    "                                self.drop,\n",
    "                                self.z_drop)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = OntoDecoder(self.in_features,\n",
    "                                    self.layer_dims_dec,\n",
    "                                    self.mask_list,\n",
    "                                    self.latent_dim,\n",
    "                                    self.neuronnum,\n",
    "                                    self.drop)\n",
    "        \n",
    "        if not dataset in ontobj.data[str(top_thresh) + '_' + str(bottom_thresh)].keys():\n",
    "            sys.exit('Available datasets are: ' + ', '.join(list(ontobj.data[str(top_thresh) + '_' + str(bottom_thresh)].keys())))\n",
    "\n",
    "        self.X = ontobj.data[str(top_thresh) + '_' + str(bottom_thresh)][dataset]\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -------------\n",
    "        mu: mean from the encoder's latent space\n",
    "        log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        sigma = torch.exp(0.5*log_var) \n",
    "        eps = torch.randn_like(sigma) \n",
    "        return mu + eps * sigma\n",
    "        \n",
    "    def get_embedding(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        embedding = mu\n",
    "        # embedding =  self.reparameterize(mu, log_var)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        mu, log_var = self.encoder(x)\n",
    "            \n",
    "        # sample from latent space\n",
    "        z = mu\n",
    "        # z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # decoding\n",
    "        reconstruction = self.decoder(z)\n",
    "            \n",
    "        return reconstruction, mu, log_var\n",
    "\n",
    "    def vae_loss(self, reconstruction, mu, log_var, data, kl_coeff):\n",
    "        kl_loss = -0.5 * torch.sum(1. + log_var - mu.pow(2) - log_var.exp(), )\n",
    "        rec_loss = F.mse_loss(reconstruction, data, reduction=\"sum\")\n",
    "        return torch.mean(rec_loss + kl_coeff*kl_loss)\n",
    "\n",
    "    def train_round(self, dataloader, lr, kl_coeff, optimizer):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -------------\n",
    "        dataloader: pytorch dataloader instance with training data\n",
    "        lr: learning rate\n",
    "        kl_coeff: coefficient for weighting Kullback-Leibler loss\n",
    "        optimizer: optimizer for training\n",
    "        \"\"\"\n",
    "        # set to train mode\n",
    "        self.train()\n",
    "\n",
    "        # initialize running loss\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # iterate over dataloader for training\n",
    "        for i, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "            # move batch to device\n",
    "            data = data[0].to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward step\n",
    "            reconstruction, mu, log_var = self.forward(data)\n",
    "            loss = self.vae_loss(reconstruction, mu, log_var, data, kl_coeff)\n",
    "            # running_loss += loss.item()\n",
    "            # running_loss = loss.item()\n",
    "            if i == 0 :\n",
    "                running_loss_min=loss.item()\n",
    "            elif running_loss_min > loss.item() :\n",
    "                    running_loss_min=loss.item()\n",
    "\n",
    "\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # zero out gradients from non-existent connections\n",
    "            for i in range(len(self.decoder.decoder)):\n",
    "                self.decoder.decoder[i][0].weight.grad = torch.mul(self.decoder.decoder[i][0].weight.grad, self.decoder.masks[i])\n",
    "\n",
    "            # perform optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # make weights in Onto module positive\n",
    "            for i in range(len(self.decoder.decoder)):\n",
    "                self.decoder.decoder[i][0].weight.data = self.decoder.decoder[i][0].weight.data.clamp(0)\n",
    "\n",
    "        # compute avg training loss\n",
    "        # train_loss = running_loss/len(dataloader)\n",
    "        train_loss = running_loss_min\n",
    "        return train_loss\n",
    "\n",
    "    def val_round(self, dataloader, kl_coeff):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -------------\n",
    "        dataloader: pytorch dataloader instance with training data\n",
    "        kl_coeff: coefficient for weighting Kullback-Leibler loss\n",
    "        \"\"\"\n",
    "        # set to eval mode\n",
    "        self.eval()\n",
    "\n",
    "        # initialize running loss\n",
    "        running_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # iterate over dataloader for validation\n",
    "            for i, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "                # move batch to device\n",
    "                data = data[0].to(self.device)\n",
    "\n",
    "                # forward step\n",
    "                reconstruction, mu, log_var = self.forward(data)\n",
    "                loss = self.vae_loss(reconstruction, mu, log_var,data, kl_coeff)\n",
    "                # running_loss += loss.item()\n",
    "                if i == 0 :\n",
    "                    running_loss_min=loss.item()\n",
    "                elif running_loss_min > loss.item() :\n",
    "                        running_loss_min=loss.item()\n",
    "\n",
    "        # compute avg val loss\n",
    "        # val_loss = running_loss/len(dataloader)\n",
    "        val_loss = running_loss_min\n",
    "        return val_loss\n",
    "\n",
    "    def train_model(self, modelpath, lr=1e-4, kl_coeff=1e-4, batch_size=128, epochs=300, log=True, log_location=\"\", **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -------------\n",
    "        modelpath: where to store the best model (full path with filename)\n",
    "        lr: learning rate\n",
    "        kl_coeff: Kullback Leibler loss coefficient\n",
    "        batch_size: size of minibatches\n",
    "        epochs: over how many epochs to train\n",
    "        log: whether run should be logged to neptune\n",
    "        **kwargs: pass the run here if log == True\n",
    "        \"\"\"\n",
    "        # train-test split\n",
    "        indices = np.random.RandomState(seed=42).permutation(self.X.shape[0])\n",
    "        X_train_ind = indices[:round(len(indices)*0.8)]\n",
    "        X_val_ind = indices[round(len(indices)*0.8):]\n",
    "        X_train, X_val = self.X[X_train_ind,:], self.X[X_val_ind,:]\n",
    "\n",
    "        # convert train and val into torch tensors\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "        # generate dataloaders\n",
    "        trainloader = FastTensorDataLoader(X_train, \n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=True)\n",
    "        valloader = FastTensorDataLoader(X_val, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=False)\n",
    "\n",
    "        val_loss_min = float('inf')\n",
    "        optimizer = optim.AdamW(self.parameters(), lr = lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "            train_epoch_loss = self.train_round(trainloader, lr, kl_coeff, optimizer)\n",
    "            val_epoch_loss = self.val_round(valloader, kl_coeff)\n",
    "            \n",
    "            if log == True:\n",
    "                progress = \"Epoch: {0}, Train Loss: {1:.4f}, Val Loss: {2:.4f}\".format(epoch+1, train_epoch_loss, val_epoch_loss)\n",
    "                with open(log_location, \"a\") as logs:\n",
    "                    logs.write(progress+\"\\n\")\n",
    "            \n",
    "            # if ((epoch+1) in [5,10,20] or (epoch+1) %50 == 0):\n",
    "            #     new_modelpath=modelpath+\"_model_epoch_\"+str(epoch+1)+\".pt\"\n",
    "            #     print('save model:'+ modelpath)\n",
    "            #     torch.save({\n",
    "            #         'epoch': epoch,\n",
    "            #         'model_state_dict': self.state_dict(),\n",
    "            #         'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #         'loss': val_epoch_loss,\n",
    "            #     }, new_modelpath)\n",
    "                \n",
    "            if ((val_epoch_loss < val_loss_min) and (epoch+1)>=280):\n",
    "                new_modelpath=modelpath+\"_best_model.pt\"\n",
    "                print('New best model!')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': val_epoch_loss,\n",
    "                }, new_modelpath)\n",
    "                val_loss_min = val_epoch_loss\n",
    "                \n",
    "            print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "    def _pass_data(self, data, output):\n",
    "        \"\"\"\n",
    "        output\n",
    "            one of 'act': pathway activities\n",
    "                    'rec': reconstructed values\n",
    "        \"\"\"\n",
    "\n",
    "        # set to eval mode\n",
    "        self.eval()\n",
    "\n",
    "        # get latent space embedding\n",
    "        with torch.no_grad():\n",
    "            z = self.get_embedding(data)\n",
    "            z = z.to('cpu').detach().numpy()\n",
    "        \n",
    "        z = np.array(np.split(z, z.shape[1]/self.neuronnum, axis=1)).mean(axis=2).T\n",
    "\n",
    "        # get activities from decoder\n",
    "        activation = {}\n",
    "        def get_activation(index):\n",
    "            def hook(model, input, output):\n",
    "                activation[index] = output.to('cpu').detach()\n",
    "            return hook\n",
    "\n",
    "        hooks = {}\n",
    "\n",
    "        for i in range(len(self.decoder.decoder)-1):\n",
    "            key = str(i)\n",
    "            value = self.decoder.decoder[i][0].register_forward_hook(get_activation(i))\n",
    "            hooks[key] = value\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reconstruction, _, _ = self.forward(data)\n",
    "\n",
    "        act = torch.cat(list(activation.values()), dim=1).numpy()\n",
    "        act = np.array(np.split(act, act.shape[1]/self.neuronnum, axis=1)).mean(axis=2).T\n",
    "        \n",
    "        # remove hooks\n",
    "        for h in hooks:\n",
    "            hooks[h].remove()\n",
    "\n",
    "        # return pathway activities or reconstructed gene values\n",
    "        if output == 'act':\n",
    "            return np.hstack((z,act))\n",
    "        if output == 'rec':\n",
    "            return reconstruction.to('cpu').detach().numpy()\n",
    "        \n",
    "\n",
    "    def get_pathway_activities(self, ontobj, dataset, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -------------\n",
    "        ontobj: instance of the class Ontobj(), should be the same as the one used for model training\n",
    "        dataset: which dataset to use for pathway activity retrieval\n",
    "        **kwargs\n",
    "        terms: if we only want to get back the activities for certain terms (should be list of ids)\n",
    "        \"\"\"\n",
    "        if self.ontology != ontobj.description:\n",
    "            sys.exit('Wrong ontology provided, should be ' + self.ontology)\n",
    "\n",
    "        data = ontobj.data[str(self.top) + '_' + str(self.bottom)][dataset].copy()\n",
    "\n",
    "        # convert data to tensor and move to device\n",
    "        data = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # retireve pathway activities\n",
    "        act = self._pass_data(data, 'act')\n",
    "\n",
    "        # if term was specified, subset\n",
    "        if 'terms' in kwargs:\n",
    "            terms = kwargs.get('terms')\n",
    "            annot = ontobj.annot[str(self.top) + '_' + str(self.bottom)]\n",
    "            term_ind = annot[annot.ID.isin(terms)].index.to_numpy()\n",
    "\n",
    "            act = act[:,term_ind]\n",
    "\n",
    "        return act\n",
    "\n",
    "\n",
    "    def get_reconstructed_values(self, ontobj, dataset, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -------------\n",
    "        ontobj: instance of the class Ontobj(), should be the same as the one used for model training\n",
    "        dataset: which dataset to use for pathway activity retrieval\n",
    "\n",
    "        **kwargs\n",
    "        rec_genes: if we only want to get back values for certain genes\n",
    "        \"\"\"\n",
    "        if self.ontology != ontobj.description:\n",
    "            sys.exit('Wrong ontology provided, should be ' + self.ontology)\n",
    "\n",
    "        data = ontobj.data[str(self.top) + '_' + str(self.bottom)][dataset].copy()\n",
    "\n",
    "        # convert data to tensor and move to device\n",
    "        data = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # retrieve pathway activities\n",
    "        rec = self._pass_data(data, 'rec')\n",
    "\n",
    "        # if genes were passed, subset\n",
    "        if 'rec_genes' in kwargs:\n",
    "            genes = kwargs.get('rec_genes')\n",
    "            onto_genes = ontobj.genes[str(self.top) + '_' + str(self.bottom)]\n",
    "            gene_ind = np.array([onto_genes.index(g) for g in genes])\n",
    "\n",
    "            rec = rec[:,gene_ind]\n",
    "\n",
    "        return rec\n",
    "\n",
    "        \n",
    "    def perturbation(self, ontobj, dataset, genes, values, output='terms', **kwargs):\n",
    "        \"\"\"\n",
    "        This function retrieves pathway activities after performing in silico perturbation\n",
    "\n",
    "        Parameters\n",
    "        -------------\n",
    "        ontobj: instance of the class Ontobj(), should be the same as the one used for model training\n",
    "        dataset: which dataset to use for perturbation and pathway activity retrieval\n",
    "        genes: a list of genes to perturb\n",
    "        values: list with new values, same length as genes\n",
    "        output: 'terms' or 'genes'\n",
    "\n",
    "        **kwargs\n",
    "        terms: if we only want to get back the activities for certain terms (should be list of ids)\n",
    "        rec_genes: if we only want to get back values for certain reconstructed genes\n",
    "        \"\"\"\n",
    "\n",
    "        if self.ontology != ontobj.description:\n",
    "            sys.exit('Wrong ontology provided, should be ' + self.ontology)\n",
    "\n",
    "        data = ontobj.data[str(self.top) + '_' + str(self.bottom)][dataset].copy()\n",
    "\n",
    "        # get indices of the genes in list\n",
    "        indices = [self.genes.index(g) for g in genes]\n",
    "\n",
    "        # replace their values\n",
    "        for i in range(len(genes)):\n",
    "            data[:,indices[i]] = values[i]\n",
    "\n",
    "        # convert data to tensor and move to device\n",
    "        data = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # get pathway activities or reconstructed values after perturbation\n",
    "        if output == 'terms':\n",
    "            res = self._pass_data(data, 'act')\n",
    "        if output == 'genes':\n",
    "            res = self._pass_data(data, 'rec')\n",
    "\n",
    "        # if term was specified, subset\n",
    "        if 'terms' in kwargs:\n",
    "            terms = kwargs.get('terms')\n",
    "            annot = ontobj.annot[str(self.top) + '_' + str(self.bottom)]\n",
    "            term_ind = annot[annot.ID.isin(terms)].index.to_numpy()\n",
    "\n",
    "            res = res[:,term_ind]\n",
    "        \n",
    "        if 'rec_genes' in kwargs:\n",
    "            genes = kwargs.get('rec_genes')\n",
    "            onto_genes = ontobj.genes[str(self.top) + '_' + str(self.bottom)]\n",
    "            gene_ind = np.array([onto_genes.index(g) for g in genes])\n",
    "\n",
    "            res = res[:,gene_ind]\n",
    "\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwo_model = OntoVAE(ontobj=onto_obj,              # the Ontobj we will use\n",
    "                    dataset='immune_cell',     # which dataset from the Ontobj to use for model training\n",
    "                    top_thresh=1000,         # which trimmed version to use\n",
    "                    bottom_thresh=30,\n",
    "                    neuronnum=3)        # which trimmed version to use     \n",
    "pwo_model.to(pwo_model.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwo_model.train_model('../data/OntoVAE',   # where to store the best model\n",
    "                     lr=1e-4,                                 # the learning rate\n",
    "                     kl_coeff=1e-4,                           # the weighting coefficient for the Kullback Leibler loss\n",
    "                     batch_size=128,                          # the size of the minibatches\n",
    "                     epochs=300,\n",
    "                     log=True,\n",
    "                     log_location=\"./immune_cell_HC_GO_loss_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('Project/02_BloodML/map4k4_clean/data/OntoVAE_best_model.pt',\n",
    "                        map_location = torch.device(pwo_model.device))\n",
    "pwo_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve pathway activities\n",
    "pwo_act = pwo_model.get_pathway_activities(ontobj=onto_obj,\n",
    "                                           dataset='immune_cell')\n",
    "pwo_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make scatterplot for two pathway activities\n",
    "onto_obj.plot_scatter(sample_annot = '../data/annot.csv',   # pandas Dataframe or path to annotation file\n",
    "                 color_by = 'celltype2',                                 # variable to use for coloring\n",
    "                 act = pwo_act,                                          # pathway activities computed from OntoVAE model\n",
    "                 term1 = 'neutrophil activation',        # term on x-axis of scatter plot\n",
    "                 term2 = 'regulation of mononuclear cell proliferation',            # term on y-axis of scatter plot\n",
    "                 top_thresh = 1000,                                      # which trimmed version to use\n",
    "                 bottom_thresh = 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In silico knockout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation(self, ontobj, dataset, genes, values, tissue=None , annot= None ,output='terms', terms=None, rec_genes=None):\n",
    "        \"\"\"\n",
    "        Retrieves pathway activities or reconstructed gene values after performing in silico perturbation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ontobj\n",
    "            instance of the class Ontobj(), should be the same as the one used for model training\n",
    "        dataset\n",
    "            which dataset to use for perturbation and pathway activity retrieval\n",
    "        genes\n",
    "            a list of genes to perturb\n",
    "        values\n",
    "            list with new values, same length as genes\n",
    "        output\n",
    "            - 'terms': retrieve pathway activities\n",
    "            - 'genes': retrieve reconstructed values\n",
    "\n",
    "        terms\n",
    "            list of ontology term ids whose values should be retrieved\n",
    "        rec_genes\n",
    "            list of genes whose values should be retrieved\n",
    "        \"\"\"\n",
    "\n",
    "        if self.ontology != ontobj.description:\n",
    "            raise ValueError('Wrong ontology provided, should be ' + self.ontology)\n",
    "\n",
    "        data = ontobj.data[str(self.top) + '_' + str(self.bottom)][dataset].copy()\n",
    "\n",
    "        # get indices of the genes in list\n",
    "        indices = [self.genes.index(g) for g in genes]\n",
    "\n",
    "        # replace their values\n",
    "        if tissue is not None:\n",
    "             anno=pd.read_csv(annot)\n",
    "             for i in range(len(genes)):\n",
    "                data[anno[anno['celltype']==tissue].index,indices[i]] = values[i]\n",
    "        else :\n",
    "             for i in range(len(genes)):\n",
    "                 data[:,indices[i]] = values[i]     \n",
    "        \n",
    "\n",
    "        # convert data to tensor and move to device\n",
    "        data = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # get pathway activities or reconstructed values after perturbation\n",
    "        if output == 'terms':\n",
    "            res = self._pass_data(data, 'act')\n",
    "        if output == 'genes':\n",
    "            res = self._pass_data(data, 'rec')\n",
    "\n",
    "        # if term was specified, subset\n",
    "        if terms is not None:\n",
    "            annot = ontobj.annot[str(self.top) + '_' + str(self.bottom)]\n",
    "            term_ind = annot[annot.ID.isin(terms)].index.to_numpy()\n",
    "\n",
    "            res = res[:,term_ind]\n",
    "        \n",
    "        if rec_genes is not None:\n",
    "            onto_genes = ontobj.genes[str(self.top) + '_' + str(self.bottom)]\n",
    "            gene_ind = np.array([onto_genes.index(g) for g in rec_genes])\n",
    "\n",
    "            res = res[:,gene_ind]\n",
    "\n",
    "        return res\n",
    "\n",
    "def wilcox_test(obj, control, perturbed, tissue=None , annot= None, direction='up', option='terms', top_thresh=1000, bottom_thresh=30):\n",
    "        \"\"\" \n",
    "        Performs paired Wilcoxon test between activities and perturbed activities.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        act\n",
    "            numpy 2D array of pathway activities \n",
    "        perturbed_act\n",
    "            numpy 2D array of perturbed pathway activities\n",
    "        direction\n",
    "            up: higher in perturbed\n",
    "            down: lower in perturbed\n",
    "        top_thresh\n",
    "            top threshold for trimming\n",
    "        bottom_thresh\n",
    "            bottom_threshold for trimming\n",
    "        option\n",
    "            'terms' or 'genes'\n",
    "        \"\"\"\n",
    "        # perform paired wilcoxon test over all terms\n",
    "        if direction == 'up':\n",
    "                alternative = 'greater' \n",
    "        elif direction == 'down':\n",
    "                 alternative = 'less' \n",
    "        elif direction == 'twoside':\n",
    "                 alternative = 'two-sided' \n",
    "        if tissue is not None:\n",
    "            anno=pd.read_csv(annot)\n",
    "            ti=anno[anno['celltype']==tissue].index\n",
    "            wilcox = [stats.wilcoxon(perturbed[ti,i], control[ti,i], zero_method='zsplit', alternative=alternative) for i in range(control.shape[1])]\n",
    "        else :           \n",
    "            wilcox = [stats.wilcoxon(perturbed[:,i], control[:,i], zero_method='zsplit', alternative=alternative) for i in range(control.shape[1])]\n",
    "        stat = np.array([i[0] for i in wilcox])\n",
    "        pvals = np.array([i[1] for i in wilcox])\n",
    "        qvals = fdrcorrection(np.array(pvals))\n",
    "\n",
    "        if option == 'terms':\n",
    "            # extract ontology annot\n",
    "            onto_annot = obj.extract_annot(top_thresh=top_thresh, bottom_thresh=bottom_thresh)\n",
    "\n",
    "            # create results dataframe \n",
    "            res = pd.DataFrame({'id': onto_annot.ID.tolist(),\n",
    "                                'term': onto_annot.Name.tolist(),\n",
    "                                'depth': onto_annot.depth.tolist(),\n",
    "                                'stat': stat,\n",
    "                                'pval' : pvals,\n",
    "                                'qval': qvals[1]})\n",
    "        \n",
    "        else:\n",
    "            # extract ontology genes\n",
    "            onto_genes = obj.extract_genes(top_thresh=top_thresh, bottom_thresh=bottom_thresh)\n",
    "\n",
    "            # create results dataframe\n",
    "            res = pd.DataFrame({'gene': onto_genes,\n",
    "                                'stat': stat,\n",
    "                                'pval' : pvals,\n",
    "                                'qval': qvals[1]})\n",
    "\n",
    "        res = res.sort_values('pval').reset_index(drop=True)\n",
    "        return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwo_act = pwo_model.get_pathway_activities(ontobj=onto_obj,\n",
    "                                           dataset='immune_cell')\n",
    "\n",
    "names = []\n",
    "\n",
    "with open('../data/functional_genes.txt', 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        # if i == 50:  \n",
    "        #     break\n",
    "        names.append(line.strip()) \n",
    "\n",
    "for gene in names[:]:  \n",
    "    if gene not in pwo.genes['1000_30']:\n",
    "        names.remove(gene)\n",
    "\n",
    "for cell in [\"Neu\"]:\n",
    "    for gene in names:\n",
    "        print(cell+\":\"+gene)        \n",
    "        pwo_ko_act= perturbation(pwo_model,ontobj=onto_obj,\n",
    "                                    dataset='immune_cell',\n",
    "                                    genes=[gene],\n",
    "                                    annot='../data/annot.csv',\n",
    "                                    tissue=cell,\n",
    "                                    values=[0]) \n",
    "        results_down = wilcox_test(    obj=onto_obj,\n",
    "                          control = pwo_act,\n",
    "                          perturbed = pwo_ko_act,\n",
    "                          direction = 'down',\n",
    "                          annot='../data/annot.csv',\n",
    "                          tissue=cell,\n",
    "                          top_thresh=1000,\n",
    "                          bottom_thresh=30)\n",
    "        results_down['gene']=gene\n",
    "\n",
    "        path=\"../results/\"+cell+\"_gene_rank_down.csv\"\n",
    "\n",
    "        results_down.to_csv(path,mode=\"a\",index=False,header=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
